# -*- coding: utf-8 -*-
"""CS4347_Assignment2_NetID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uLL1gsfjWsD7GMMUEv3MlTy_FDpJvrxJ

# Assignment 2 - Logistic Regression
> **FULL MARKS = 5*20 = 100**

In this assignment, you are going to implement your own logistic Regression function. Please notice no library versions of logistic regression are allowed. Follow the instructions, you will need to fill the blanks to make it functional. The process is similar to the previous assignment. 
> **Note**

This assignment is adapted from the coursera online machine learning course. You can refer it here: https://www.coursera.org/learn/machine-learning
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

# enter the foldername in your Drive where you have saved the unzipped
# 'cs4347' folder containing the '.py', 'classifiers' and 'datasets'
# folders.
# e.g. 'assignments/cs4347/'
FOLDERNAME = 'cs4347/'

assert FOLDERNAME is not None, "[!] Enter the foldername."

# %cd drive/My\ Drive
# %cp -r $FOLDERNAME ../../
# %cd ../../

"""***Initialization***"""

# load required library
import matplotlib.pyplot as plt
import numpy as np

"""Load data
_________

This data contains two columns ['exam1','exam2', 'admitted'], we are trying to predict whether admitted by scores of exam1 and exam2.
"""

from cs4347.assignment_datasets import assign2

data = assign2()

"""Visualize data
___________
"""

# draw raw data
def draw_data(data):
    # We mark data points with admitted as 1, others as 0 for better nummerical operation
    positive = data[data.admitted.isin(['1'])]  # 1
    negative = data[data.admitted.isin(['0'])]  # 0
    # initialize plot
    fig, ax = plt.subplots(figsize=(6,5))
    
    #########################################################################
    # TODO:                                                                 #
    # 1. make a scatter plot of the raw data                                #
    # 2. set title for the plot                                             #
    # 3. set label for x,y axis                                             #
    # Note, this scatter plot has two different type of points              #
    #########################################################################
    ax.scatter(positive.iloc[:, 0], positive.iloc[:, 1], s=15, label='Admitted') #iloc == <select row, select column>
    ax.scatter(negative.iloc[:, 0], negative.iloc[:, 1], s=15, label='Not Admitted')
    ax.legend()
    ax.set_xlabel('Exam 1 Grade')
    ax.set_ylabel('Exam 2 Grade')
    #########################################################################
    #                       END OF YOUR CODE                                #
    #########################################################################

    # show plot
    plt.show()

draw_data(data)

"""Sigmoid function
________________
"""

# define sigmoid function
# math: refer to https://en.wikipedia.org/wiki/Sigmoid_function or slides
def sigmoid(z):
    #########################################################################
    # TODO:                                                                 #
    # 1. implement the sigmoid function over input z
    #########################################################################
    s = 1 / (1 + np.exp(-z))
    #########################################################################
    #                       END OF YOUR CODE                                #
    #########################################################################
    
    return s

"""Cost function
____________
"""

# define cost function with sigmoid function
def cost(theta, X, y):
    #########################################################################
    # TODO:                                                                 #
    # 1. implement the cross entropy loss function with sigmoid             #
    # Hint: Use @ to perform matrix element-wise multiplication             #
    #########################################################################
    theta = sigmoid(np.dot(X, theta))
    cost1 = np.dot(-y.T, np.log(theta))
    cost2 = np.dot((1 - y).T, np.log(1 - theta))
    co = cost1 - cost2
    #########################################################################
    #                       END OF YOUR CODE                                #
    #########################################################################
    return np.mean(co)

"""calculate gradients
________________
"""

# the gradient of the cost is a vector of the same length as Î¸ where the jth element (for j = 0, 1, . . . , n)
def gradient(theta, X, y):
    #########################################################################
    # TODO:                                                                 #
    # 1. calculate the gradients using theta and sigmoid                    #
    # Hint: X may need to be transposed to do matrix operation              #
    #########################################################################
    m = X.shape[0]
    grad = (1 / m) * np.dot(X.T, sigmoid(np.dot(X, theta)) - y)
    #########################################################################
    #                       END OF YOUR CODE                                #
    #########################################################################
    return grad

# predict for new X
def predict(theta, X):
 
    #########################################################################
    # TODO:                                                                 #
    # 1. predict the value using theta and sigmoid                          #
    # 2. convert the predicted value to 0/1                                 #
    # That's how it is called Logistic regression                           #
    #########################################################################
    probability = np.array(sigmoid(np.dot(X, theta)))
    predict_labels = probability >= 0.5
    #########################################################################
    #                       END OF YOUR CODE                                #
    #########################################################################

    return predict_labels

"""Calling functions
________
"""

# read the data
data = assign2()
# print head to check if data is correct
print(data.head())
# statistic information
data.describe()

# draw raw data
draw_data(data)

# add a ones column - this makes the matrix multiplication work out easier
if 'Ones' not in data.columns:
    data.insert(0, 'Ones', 1)

# set X (training data) and y (target variable)
X = data.iloc[:, :-1].values  # Convert the frame to its Numpy-array representation.
y = data.iloc[:, -1].values  # Return is NOT a Numpy-matrix, rather, a Numpy-array.

theta = np.zeros(X.shape[1])

X.shape, theta.shape, y.shape

# invoke optimization library
import scipy.optimize as opt

result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))
result

# see the final cost
cost(result[0], X, y)

# calculate the accuracy of the model for the traning set
final_theta = result[0]
predictions = predict(final_theta, X)
correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]
accuracy = sum(correct) / len(X)
accuracy
print("acc", accuracy)

"""**Decsion boudary** [optional, for reference]"""

# plot decsion boudary
positive = data[data.admitted.isin(['1'])]  # 1
negetive = data[data.admitted.isin(['0'])]  # 0

x1 = np.arange(130, step=0.1)
x2 = -(final_theta[0] + x1*final_theta[1]) / final_theta[2]

fig, ax = plt.subplots(figsize=(8,5))
ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
ax.plot(x1, x2)
ax.set_xlim(0, 130)
ax.set_ylim(0, 130)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_title('Decision Boundary')
plt.show()